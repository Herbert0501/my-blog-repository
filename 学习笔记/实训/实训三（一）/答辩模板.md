
## 小组介绍

接下来由我代表我们组参加进行展示。 我们小组由五人组成，我主要负责对整体项目设计与开发、项目指导和管理，李思黎和蒙婷同学分别负责机器视觉和 Web 界面的开发和文档的辅助工作，梁承志同学负责主要负责数据的采集、数据库设计与实现以及开发，熊康杰主要负责项目的测试与测试文档的编写。


## 项目概述

本系统系统分为 Web 端和计算机视觉 2 部分。通过摄像头实时拍摄到的画面，对老人 的情感、摔倒、入侵、互动等事件进行检测与分析。一旦上述事件发生，该事件会被实时地 更新在 Web 界面中，从而管理人员可以迅速做出反应。系统 Web 端仅供系统管理员使用。 在对项目的问题、范围等作出初步分析后，我们制定了项目章程，以指导项目后续的开发、 管理等工作。


## 主要算法

1. **初始化与加载编码**：
    
    - `FaceUtil`类初始化时可以加载已存储的人脸编码和名称，以便后续识别。
2. **人脸检测**：
    
    - `get_face_location`方法使用`face_recognition`库检测输入图像中的人脸，并返回其坐标。
    - `face_recognition.face_locations`函数检测并返回人脸位置的边界框。
3. **人脸识别**：
    
    - `get_face_location_and_name`方法负责识别图像中的人脸并返回相应的名称。
    - 使用`face_recognition.compare_faces`方法与已知编码进行比对，并使用投票机制确定识别结果。
4. **编码保存**：
    
    - `save_embeddings`方法遍历提供的图像路径，提取人脸编码，并保存到指定文件。
    - 对于每张图像，检查其人脸编码数量，确保每张图像仅包含一个人脸；否则删除该图像并记录警告。

### 关键算法

- **HOG（方向梯度直方图）和 CNN（卷积神经网络）**：通过选择不同的检测方法（`hog`或`cnn`）来实现人脸检测，HOG是一种经典的计算机视觉算法，而CNN是一种现代的深度学习方法。
- **人脸编码**：使用深度学习模型生成每个检测到人脸的128维嵌入向量，利用这些向量进行相似性比较，实现人脸识别。


## **MiniVGGNet** 模型结构相对简单，适合用于图像分类任务

1. **模型初始化**：
    
    - 使用 `Sequential` 创建顺序模型，便于逐层添加网络组件。
2. **输入形状**：
    
    - 输入形状由宽度、长度、深度（通道数）组成，默认设置为 `channels last` 格式（高度、宽度、深度）。
    - 若使用 `channels first` 格式，则需相应调整输入形状。
3. **卷积层**：
    
    - 每组层包括两个卷积层、ReLU 激活层、批量归一化层、最大池化层和 Dropout 层，帮助提取图像特征并防止过拟合。
4. **展平和全连接层**：
    
    - `Flatten` 层将卷积层的输出展平，以便输入到全连接层中。
    - `Dense` 层用于分类任务，最后一层的神经元数量等于目标类别的数量，激活函数使用 Softmax。
5. **Dropout**：
    
    - Dropout 层通过随机丢弃一定比例的神经元（25%或50%）来减少过拟合。

### 总结

这个 **MiniVGGNet** 模型结构相对简单，适合用于图像分类任务，特别是在小型数据集上进行快速实验。可以通过调整网络的深度、卷积核数量或添加更多层来进行更复杂的任务。

## 识别并跟踪义工和老年人的面部

### 主要算法步骤

1. **输入视频流的获取**：
    
    - 如果指定了视频文件名，程序从该文件读取视频；否则，它会从默认摄像头捕获实时视频流。
2. **面部识别模型加载**：
    
    - 使用预训练的面部识别模型（HOG模型），该模型通过 `FaceUtil` 类进行加载和初始化，用于识别视频帧中的人脸。
3. **帧处理**：
    
    - 在每一帧中：
        - 调整帧的大小以适应处理需求。
        - 使用面部识别模型检测人脸位置和姓名。
        - 在帧上绘制人脸框，并根据不同身份（义工、老人、员工）使用不同颜色进行标记。
4. **义工和老年人的重心计算**：
    
    - 对于检测到的每个人，计算他们的重心（即人脸框的中心），并根据身份分类保存义工和老年人的重心位置。
5. **互动判断**：
    
    - 通过比较义工与老年人之间的距离来判断是否存在互动：
        - 计算义工与老年人之间的像素距离。
        - 将像素距离转换为实际距离（厘米），并与设定的距离阈值（100厘米）进行比较。
        - 如果距离小于阈值，则视为有互动，绘制连接线并记录事件。
6. **事件记录与通知**：
    
    - 当检测到互动时：
        - 生成事件描述并将其存入数据库。
        - 发送事件信息到指定的服务器地址。
7. **图像展示**：
    
    - 将处理后的帧在窗口中显示，实时展示义工与老年人之间的互动情况。
    - 用户可以按 `ESC` 键退出程序。

### 关键技术

- **面部识别**：使用HOG特征和机器学习模型进行人脸检测和识别。
- **距离计算**：通过欧几里得距离计算义工与老年人之间的空间关系，结合实际物理尺寸进行判断。
- **图像处理**：使用OpenCV库进行实时图像处理和展示。


## 人脸采集

### 1. **参数解析**

使用 `argparse` 库解析命令行参数，获取用户ID和图像保存目录。这些参数用于后续图像的存储路径。

### 2. **摄像头设置**

使用 OpenCV 的 `VideoCapture` 类打开默认摄像头，并设置视频的宽度和高度。这为图像采集做好准备。

### 3. **人脸检测**

程序通过 `FaceUtil` 类来实现人脸检测。在每一帧图像中，程序会调用 `get_face_location` 方法，返回人脸的位置坐标。

### 4. **帧处理循环**

在不断循环中，程序执行以下步骤：

- 读取摄像头的图像帧，并进行镜像翻转。
- 检测人脸并绘制人脸框。
- 根据人脸检测的结果做出不同的处理：
    - **未检测到人脸**：播放音频提示用户。
    - **检测到一个人脸**：播放音频提示用户可以开始采集图像，并跳出循环以进入图像采集阶段。
    - **检测到多个人脸**：播放音频提示用户检测到多张人脸，并设置错误状态以防止继续采集。

### 5. **图像采集**

当检测到一个人脸后，程序会进入图像采集阶段：

- 遍历定义的动作列表（如眨眼、微笑等），为每个动作播放提示音。
- 对于每个动作，程序采集15帧图像：
    - 读取图像并翻转。
    - 绘制人脸框和当前动作提示文本。
    - 保存原始图像到指定路径。

### 6. **结束与清理**

在完成图像采集后，程序释放摄像头并关闭所有OpenCV窗口，确保资源得到释放。


## 禁止区域检测

### 1. **物体检测算法**

- **深度学习模型**：使用了 MobileNet SSD（Single Shot Detector）模型，该模型基于深度学习进行实时物体检测。它可以检测多个类别的物体，包括行人。
- **Blob检测**：将图像转换为“blob”形式（即图像的某种形式），并通过深度学习网络进行处理，以获取检测结果。

### 2. **对象追踪算法**

- **Dlib 追踪器**：采用了 Dlib 的相关追踪器（`dlib.correlation_tracker`），用于追踪检测到的人员对象。在物体检测后，创建追踪器来持续追踪每个检测到的对象。
- **质心追踪**：使用 `CentroidTracker` 类来管理和跟踪每个对象的质心，计算对象在帧之间的移动。

### 3. **运动检测和计数**

- **方向检测**：通过分析对象质心的 y 坐标变化，判断对象是向上移动还是向下移动，从而区分对象的进入和离开。
- **计数机制**：当对象的运动方向与预设的中心线（屏幕中间的水平线）相交时，记录进入或离开的数量。确保每个对象只被计数一次，使用 `counted` 标志位进行管理。

### 4. **可视化和反馈**

- **实时显示**：程序实时显示监控区域内的人员活动，包括每个对象的ID、状态（进入或离开），并在视频帧上绘制检测到的对象的边界框和质心。
- **截图功能**：在人员进入禁止区域时自动截图，以供后续分析。

### 5. **多线程与性能优化**

- **帧跳过**：通过设置 `skip_frames`，每隔一定帧进行物体检测，减少处理负担，提高性能。


## 陌生人脸检测

### 1. 初始化和配置

- **导入库和模块**：导入必要的库，如OpenCV、NumPy、TensorFlow、PIL等。
- **日志记录**：设置日志记录，用于记录程序的运行信息。
- **参数解析**：使用`argparse`解析命令行参数，允许用户指定输入视频文件。
- **模型路径和常量**：定义面部识别和表情识别模型的路径、输出路径和常量值。

### 2. 加载模型

- **人脸识别模型**：通过自定义的`FaceUtil`类加载人脸识别模型。
- **表情识别模型**：加载使用Keras构建的表情识别模型。

### 3. 视频流处理

- **打开视频流**：从摄像头或视频文件中读取帧。
- **图像预处理**：
    - 将图像翻转（如果不是视频文件）。
    - 调整图像大小。
    - 转换为灰度图像，以便于人脸检测和表情识别。

### 4. 人脸检测

- **检测人脸**：调用`FaceUtil`类中的`get_face_location_and_name`方法，返回人脸的位置和名称。
- **绘制人脸框**：在检测到的人脸周围绘制矩形框，使用不同的颜色表示不同类型的人（老年人、员工、志愿者）。

### 5. 陌生人检测

- **识别陌生人**：如果检测到的人脸名称为"Unknown"，启动计时器。如果在规定的时间内仍然检测到陌生人，记录事件并保存快照。

### 6. 表情识别

- **提取ROI**：对于检测到的老年人，提取其面部区域，并调整到适合模型输入的大小。
- **进行表情预测**：使用加载的表情识别模型预测该区域的表情（微笑或中性）。
- **记录微笑事件**：如果检测到微笑，并且满足时间条件，记录事件并保存快照，同时向外部API发送请求。

### 7. 显示结果

- **绘制标签**：在图像中绘制识别到的名称和表情标签。
- **显示处理后的图像**：使用OpenCV的`imshow`方法显示当前帧。

### 8. 清理资源

- **释放摄像头和关闭窗口**：在程序结束时，释放视频资源并关闭所有OpenCV窗口。

### 主要算法的关键点

- **人脸检测算法**：依赖于`FaceUtil`类，可能使用了HOG（方向梯度直方图）或其他深度学习方法来进行人脸定位。
- **表情识别算法**：使用了卷积神经网络（CNN），对面部区域进行分类，识别出“微笑”或“中性”表情。